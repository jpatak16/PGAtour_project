---
title: "PGA Tour Project"
author: "Jeremy Patak"
date: '2022-06-08'
output: html_document
---
```{r setup, echo=FALSE}
pacman::p_load(tidyverse, knitr, kableExtra, lubridate, binsreg)

options(width = 100)
knitr::opts_chunk$set(echo = FALSE) ## dont show code
knitr::opts_chunk$set(warning = FALSE) ## Limit warnings
knitr::opts_chunk$set(message = FALSE) ## Limit warnings

data = read.csv('all_tourn.csv')
data = data %>% filter(start_date<'2022-06-02')
```

# Introduction

The Masters Tournament is one of four majors on the PGA Tour each year, and widely considered the biggest and best golf tournament in the world. I wanted to see if I could predict the results of that tournament using data science. The most common gambling bets you can make in golf are tournament winner, top-10 finisher, and making/missing the cut, so I decided these would be the things I try to predict. One of my questions will be if any of these three outcomes are easier or harder to predict than the others. I also am interested in what features are best at predicting these outcomes. 

I am going to attempt to answer these questions by predicting a mean and variance for each golfer participating in the Masters Tournament, then use those values to run many simulations and see how often each golfer wins, places top 10, and makes the cut.

# Data

To get my data, I webscraped the PGA Tour leaderboards from [ESPN's website](https://www.espn.com/golf/leaderboard). Each tournament was a different webpage, and I aggregated those pages into one csv that I loaded into this notebook. I collected all the individual tournaments from each of the past 16 seasons (starting in the 2007 season until present day). The table below is a random sample preview of how the collected data looks. You can look at my [GitHub repo](https://github.com/jpatak16/PGAtour_project) for this project to see more details about how the data was scraped. 

```{r table preview}
#preview table
set.seed(3)
data[sample(1:nrow(data)), ] %>% head(n=5)%>%
  kable('html', row.names = FALSE) %>% kable_styling('striped') %>% scroll_box(width = '100%')
```

Since I am trying to make predictions for the yearly Masters Tournament, I need to manipulate this data set into one that I can use to run predictive models on. I need one row for each participant for each year's Masters Tournament, then I will need to create a set of predictors using the data I scraped from ESPN.

The first thing to think about when designing our data set is that we want to predict how golfers do compared to each other, rather than compared to par or total score. This is because due to course difficulty, weather conditions, or any other sort of external variation, a score that is good enough to place top-5 at one tournament may not be good enough to make the cut at the next. Because of this, we want our score variables to be in reference to the field of other golfers.

When calculating these scores, we can compare the player's round score to the mean round score or the median round score. 

```{r mean vs median}
#create table with means and medians of every round of every tournament in our data
round_summ = data %>% group_by(TID) %>%
  summarise(R1mean = mean(R1, na.rm=TRUE), R1med = median(R1, na.rm = TRUE),
            R2mean = mean(R2, na.rm=TRUE), R2med = median(R2, na.rm = TRUE),
            R3mean = mean(R3, na.rm=TRUE), R3med = median(R3, na.rm = TRUE),
            R4mean = mean(R4, na.rm=TRUE), R4med = median(R4, na.rm = TRUE))

#plot relationship between a rounds mean and median
ggplot(round_summ, aes(R1mean, R1med)) +
  geom_point() + geom_abline(slope=1, intercept = 0) + geom_smooth(method = 'lm') +
  xlab('Round 1 Means') + ylab('Round 1 Medians') +
  scale_x_continuous(limits = c(67,77), breaks = seq(67,77,2)) + 
  scale_y_continuous(limits = c(67,77), breaks = seq(67,77,2)) +
  ggtitle('Round Means vs Medians', 
          subtitle = 'The blue line is a line of best fit. The black line represents where mean=median.')
```

As you can see in the plot above, they are very closely related to each other, but means tend to be higher estimates than medians. Only Round 1 means/medians are plotted above, but all rounds displayed this same pattern. This suggest that the distribution of scores is slightly skewed towards higher scores. You will see that this is true soon, but for the purposes of generating golfer round scores relative to other golfers, I will use the median since it better accounts for outliers. 

```{r adjust score reference points}
#change scores in data set to be relative to the rest of the golfers competing in the same round
data2 = left_join(data, round_summ, by='TID') %>% mutate(R1 = R1 - R1med,
                                                          R2 = R2 - R2med,
                                                          R3 = R3 - R3med,
                                                          R4 = R4 - R4med) %>%
  select(-c(R1mean, R1med, R2mean, R2med, R3mean, R3med, R4mean, R4med))


```

The next thing we want to consider is whether or not we think that score distributions are statistically similar for each round. I have a good feeling that they are not, because after Round 2 of most tournaments, roughly half of the field is "cut" and do not play the next two rounds. Since the golfer in the middle of the distribution of Rounds 1 & 2 should now theoretically be at the bottom of the distribution in Rounds 3 & 4, it seems like we would want to estimate different scores for them depending what round they are in. 

```{r plot round distributions}
R12 = append(round_summ$R1med, round_summ$R2med)
R34 = append(round_summ$R3med, round_summ$R4med)

rd = data.frame(x = 1, R12 = R12, R34 = R34) %>% 
  pivot_longer(-x, names_to = 'Rounds', values_to = 'Scores') %>%
  select(-x)

ggplot(rd, aes(x=Scores, fill=Rounds, color = Rounds)) +
  geom_density(alpha = .4, bw=1) +
  geom_vline(xintercept = mean(R12), color = 'pink') + 
  geom_vline(xintercept = mean(R34, na.rm = TRUE), color = 'lightblue') +
  scale_fill_discrete(labels = c('1 & 2', '3 & 4')) + 
  scale_color_discrete(labels = c('1 & 2', '3 & 4')) + 
  xlim(c(65,78))
```

We can visually see that scores in Rounds 3 and 4 appear to be lower than in Rounds 1 and 2. (We also see that the distributions are slightly left skewed, as mentioned earlier). We will run a Kolmogorov-Smirnov Test on the distributions to see if they are statistically different from each other. 

```{r ks test}
ks.test(R12, R34, alternative = 't')

rm(R12, R34, rd)
```

Our p-value is easily significant at the .01 level, so from this test we know that scores in Rounds 3 and 4 are not drawn from the same distribution as scores from Rounds 1 and 2. This confirms that we should make different mean estimates for golfers in the later two rounds than in the first two rounds.

```{r create data set}
#create a df with one row for each player at each masters tournament
MT = data2 %>% filter(grepl('Masters', tournament)) %>% 
  filter(start_date != '2007-04-05') %>%
  filter(start_date != '2020-11-12') %>%
  select(c(2, 15, 21, 7:10))

#create our outcome variables
#we will average R1 and R2 for one y var and average R3 and R4 for the other y var
MT$R12 = ifelse(is.na(MT$R2), MT$R1, (MT$R1 + MT$R2)/2)
MT$R34 = ifelse(is.na(MT$R4), MT$R3, (MT$R3 + MT$R4)/2)

#remove round scores
MT = MT %>% select(1, 2, 3, 8, 9)
```

We set up our data frame by having one row for each golfer in each Masters Tournament. We will have two outcome variables in each row, one for the mean of the first two rounds one one for the mean of the second two rounds. Remember, all of our scores now are relative to the median golfer in that round, and not the total strokes or relative to par. 

I excluded the first year's (2007) Masters Tournament from our data set because I want to use the year leading up to each Masters as predictors, and my data doesn't include 2006 data, so we'd have no predictors for it. I also excluded the 2020 Masters since that season was heavily affected by COVID and caused cancellation of many tournaments leading up to the Masters, as well as the Masters being delayed 7 months. 

Now we need to create features that can help predict our outcome variables. 

```{r create a predictor}
#create an empty column to fill with data
MT$cuts_percent_last_year = NA
for(i in 1:nrow(MT)){
  #grab player and start date from each observation
  name = MT$player[i]
  date = MT$start_date[i]
  one_yr_prior = ymd(date) - years(1)
  #get a list of tournaments the player has played in within a year of masters start date
  tourns = data2 %>% filter(player == name) %>% filter(start_date < date) %>% filter(start_date > one_yr_prior)
  #find number of tournaments played in
  played_in = nrow(tourns)
  #find number of cuts made
  cuts_made = tourns %>% filter(!is.na(R1)) %>% filter(cut == 'FALSE') %>% 
    filter(withdraw == 'FALSE') %>% filter(dq == 'FALSE')
  more = tourns %>% filter(dq == 'TRUE') %>% filter(!is.na(R3))
  cuts_made = rbind(cuts_made, more) %>% nrow()
  #add cuts made percentage to df
  MT$cuts_percent_last_year[i] = cuts_made / played_in
}
```


